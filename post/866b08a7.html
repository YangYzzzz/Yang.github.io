<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>『深度学习』动手学深度学习——阅读笔记1 | Yang's CS World</title><meta name="author" content="Yang"><meta name="copyright" content="Yang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="『深度学习』动手学深度学习——阅读笔记1❀目录❀ 『深度学习』动手学深度学习——阅读笔记1 『深度学习』动手学深度学习——阅读笔记2 零. 前言动手学AI电子书链接。本书阅读于24年寒假，每天上午一章节，随阅读随记录笔记，记录的不详尽或复制粘贴原文之处敬请谅解。By the way，这是笔者推荐深度学习入门最好的指导书。 一.引言1. 四大基石 数据  模型  目标函数  优化算法 —— 基于梯度">
<meta property="og:type" content="article">
<meta property="og:title" content="『深度学习』动手学深度学习——阅读笔记1">
<meta property="og:url" content="https://yangyzzzz.github.io/post/866b08a7.html">
<meta property="og:site_name" content="Yang&#39;s CS World">
<meta property="og:description" content="『深度学习』动手学深度学习——阅读笔记1❀目录❀ 『深度学习』动手学深度学习——阅读笔记1 『深度学习』动手学深度学习——阅读笔记2 零. 前言动手学AI电子书链接。本书阅读于24年寒假，每天上午一章节，随阅读随记录笔记，记录的不详尽或复制粘贴原文之处敬请谅解。By the way，这是笔者推荐深度学习入门最好的指导书。 一.引言1. 四大基石 数据  模型  目标函数  优化算法 —— 基于梯度">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yangyzzzz.github.io/img/123230344_p0_master1200.jpeg">
<meta property="article:published_time" content="2024-10-29T10:04:51.000Z">
<meta property="article:modified_time" content="2024-10-30T02:17:54.152Z">
<meta property="article:author" content="Yang">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="阅读">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yangyzzzz.github.io/img/123230344_p0_master1200.jpeg"><link rel="shortcut icon" href="/img/avatar.jpeg"><link rel="canonical" href="https://yangyzzzz.github.io/post/866b08a7.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?d431d91edffce9d7bd7ade045956b24b";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":5,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":10000,"languages":{"author":"作者: Yang","link":"链接: ","source":"来源: Yang's CS World","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '『深度学习』动手学深度学习——阅读笔记1',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-10-30 10:17:54'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="/css/light.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="/css/windmill.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="/css/universe.css"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">51</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">20</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/gallery/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk/"><i class="fa-fw fas fa-comments"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/123230344_p0_master1200.jpeg')"><nav id="nav"><span id="blog-info"><a href="/" title="Yang's CS World"></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/gallery/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk/"><i class="fa-fw fas fa-comments"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">『深度学习』动手学深度学习——阅读笔记1</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-10-29T10:04:51.000Z" title="发表于 2024-10-29 18:04:51">2024-10-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-10-30T02:17:54.152Z" title="更新于 2024-10-30 10:17:54">2024-10-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/">计算机科学</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/">深度学习基础</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>14分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="『深度学习』动手学深度学习——阅读笔记1"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/post/866b08a7.html#post-comment"><span class="gitalk-comment-count"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="『深度学习』动手学深度学习——阅读笔记1"><a href="#『深度学习』动手学深度学习——阅读笔记1" class="headerlink" title="『深度学习』动手学深度学习——阅读笔记1"></a>『深度学习』动手学深度学习——阅读笔记1</h1><p>❀<strong>目录</strong>❀</p>
<p><strong><em><a href="https://yangyzzzz.github.io/post/866b08a7.html">『深度学习』动手学深度学习——阅读笔记1</a></em></strong></p>
<p><strong><em><a href="https://yangyzzzz.github.io/post/52e1e34e.html">『深度学习』动手学深度学习——阅读笔记2</a></em></strong></p>
<h2 id="零-前言"><a href="#零-前言" class="headerlink" title="零. 前言"></a>零. 前言</h2><p><strong><em><a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/">动手学AI电子书链接</a></em></strong>。本书阅读于24年寒假，每天上午一章节，随阅读随记录笔记，记录的不详尽或复制粘贴原文之处敬请谅解。By the way，这是笔者推荐<strong>深度学习</strong>入门最好的指导书。</p>
<h2 id="一-引言"><a href="#一-引言" class="headerlink" title="一.引言"></a>一.引言</h2><h3 id="1-四大基石"><a href="#1-四大基石" class="headerlink" title="1. 四大基石"></a>1. 四大基石</h3><ol>
<li><p>数据</p>
</li>
<li><p>模型</p>
</li>
<li><p>目标函数</p>
</li>
<li><p>优化算法 —— 基于梯度下降</p>
</li>
</ol>
<h3 id="2-机器学习分类"><a href="#2-机器学习分类" class="headerlink" title="2. 机器学习分类"></a>2. 机器学习分类</h3><ol>
<li>有监督学习(分类，回归，序列学习—时间序列预测），下流任务丰富多样</li>
<li>无监督学习</li>
<li>强化学习(环境—动作—奖励 三元组)</li>
</ol>
<h2 id="二-数学基础"><a href="#二-数学基础" class="headerlink" title="二. 数学基础"></a>二. 数学基础</h2><h3 id="1-线性代数"><a href="#1-线性代数" class="headerlink" title="1. 线性代数"></a>1. 线性代数</h3><ol>
<li><p>数据形式</p>
<ul>
<li><p>标量</p>
</li>
<li><p>向量</p>
</li>
<li><p>矩阵</p>
</li>
<li><p>张量</p>
</li>
</ul>
</li>
<li><p>运算</p>
<ul>
<li><p>矩阵元素级别运算</p>
</li>
<li><p>向量间点积</p>
</li>
<li><p>矩阵 * 向量</p>
</li>
<li><p>矩阵 * 矩阵</p>
</li>
</ul>
</li>
<li><p>范数：衡量向量的“大小”</p>
<ul>
<li><p>L1</p>
</li>
<li><p>L2（欧氏距离）</p>
</li>
<li><p>余弦距离</p>
</li>
</ul>
</li>
</ol>
<h3 id="2-微分"><a href="#2-微分" class="headerlink" title="2. 微分"></a>2. 微分</h3><p>构建计算图来保存每个标量在反向传播后的梯度，梯度的计算运用偏导和链式法则。</p>
<p><strong>梯度</strong>：<script type="math/tex">f(x)</script>中对每一个自变量x分值的微分，结果为一个向量</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/YangYzzzz/image-store@main/fix-dir%2FTypora%2Ftypora-user-images%2F2024%2F10%2F29%2F18-07-53-709514277eb9778a29ffe86ed4c6f899-image-20240115220027364-10427f.png" alt="image-20240115220027364"></p>
<p><strong>链式法则</strong>：见淑芬</p>
<p><strong>分离计算图</strong>：y.detach() 将y从计算图中分离，将其看为常量，不会在经过y向后传播到其他参数。</p>
<p>一般而言，函数都是”多对1”的，即向量到标量的映射，向量到向量的映射也可以被拆解为多个向量到标量的映射。</p>
<p> torch 支持自动微分，详细请参见<code>mlsys</code>部分。</p>
<h3 id="3-概率"><a href="#3-概率" class="headerlink" title="3. 概率"></a>3. 概率</h3><p>随机变量，条件概率，联合概率，Bayes定理，期望与方差。</p>
<h2 id="三-线性神经网络"><a href="#三-线性神经网络" class="headerlink" title="三. 线性神经网络"></a>三. 线性神经网络</h2><p>最简单的单层网络结构。</p>
<h3 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1. 线性回归"></a>1. 线性回归</h3><p>拥有解析解，可通过牛顿最小二乘等方法求解，然而更多的深度网络没有解析解，而只能通过梯度下降逼近的方式逼近最小值。</p>
<h3 id="2-全连接层"><a href="#2-全连接层" class="headerlink" title="2. 全连接层"></a>2. 全连接层</h3><p>最基础的线性模型。</p>
<h3 id="3-softmax"><a href="#3-softmax" class="headerlink" title="3.  softmax"></a>3.  softmax</h3><p>激活函数，无参数，用于分类，即使不是线性变换，但也是输入经过仿射变换得到，因此也是线性模型。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/YangYzzzz/image-store@main/fix-dir%2FTypora%2Ftypora-user-images%2F2024%2F10%2F29%2F18-56-48-03443e231ee8a7716f80e04fc68bec3d-image-20240116174615937-a78584.png" alt></p>
<p>在<code>pytorch</code>实现中，我们没有将softmax概率传递到损失函数中， 而是在交叉熵损失函数中传递未规范化的原始预测，在内部计算softmax及其对数， 这是一种聪明方式。</p>
<h3 id="4-仿射变换的概念"><a href="#4-仿射变换的概念" class="headerlink" title="4. 仿射变换的概念"></a>4. 仿射变换的概念</h3><p>相比之下，仿射变换是一种线性变换，它包括缩放、旋转、平移和切变等操作。在神经网络中，通常所说的仿射变换通常指的是某层的输出是输入的加权和加上偏置项，数学上表示为：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/YangYzzzz/image-store@main/fix-dir%2FTypora%2Ftypora-user-images%2F2024%2F10%2F29%2F18-57-35-f127c12bb5558cbae24f907cae9a35bc-image-20240116190703275-030613.png" alt></p>
<h2 id="四-多层感知机"><a href="#四-多层感知机" class="headerlink" title="四. 多层感知机"></a>四. 多层感知机</h2><p>深层网络的基本原理与技巧。</p>
<h3 id="1-训练误差与泛化误差"><a href="#1-训练误差与泛化误差" class="headerlink" title="1. 训练误差与泛化误差"></a>1. 训练误差与泛化误差</h3><p><em>训练误差</em>（training error）是指， 模型在训练数据集上计算得到的误差。 <em>泛化误差</em>（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。</p>
<h3 id="2-欠拟合与过拟合"><a href="#2-欠拟合与过拟合" class="headerlink" title="2. 欠拟合与过拟合"></a>2. 欠拟合与过拟合</h3><p>欠拟合：表现为训练误差和验证误差都很严重， 但它们之间仅有一点差距，意味着模型表达能力不足，无法学习模式，但是我们有理由相信可以用一个更复杂的模型降低训练误差</p>
<p>（重点）过拟合：模型在训练数据上拟合的比在潜在分布中更接近的现象称作过拟合，即训练误差很低，但是训练误差与泛化误差的差值较大，解决过拟合的方法称为<em>正则化</em></p>
<h3 id="3-影响模型泛化的三个因素"><a href="#3-影响模型泛化的三个因素" class="headerlink" title="3. 影响模型泛化的三个因素"></a>3. 影响模型泛化的三个因素</h3><ol>
<li>可调整参数的数量。当可调整参数的数量（有时称为<em>自由度</em>）很大时，模型往往更容易过拟合。</li>
<li>参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。</li>
<li>训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。</li>
</ol>
<h3 id="4-正则化技术"><a href="#4-正则化技术" class="headerlink" title="4. 正则化技术"></a>4. 正则化技术</h3><p>在模型训练技术上提高泛化性的技术，经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。何为简单：</p>
<ol>
<li><p>权重衰减法：函数约接近0越简单，即令在损失函数中添加正则惩罚项，令权重尽可能趋于0。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/YangYzzzz/image-store@main/fix-dir%2FTypora%2Ftypora-user-images%2F2024%2F10%2F29%2F18-58-03-04b20b11c2f997d31086a08ba7885a70-image-20240117160415813-9173f2.png" alt></p>
</li>
<li><p>暂退法(dropout)：平滑性，越平滑越简单，函数越能对输入的微小变化不敏感越简单。因此每轮会令部分神经元置0。注意在测试时我们使用完整的模型，不使用dropout。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/YangYzzzz/image-store@main/fix-dir%2FTypora%2Ftypora-user-images%2F2024%2F10%2F29%2F18-58-32-14e2ea1e572bd00f378d4136b053e7f5-image-20240117160332773-e443be.png" alt></p>
</li>
</ol>
<h3 id="5-训练过程"><a href="#5-训练过程" class="headerlink" title="5. 训练过程"></a>5. 训练过程</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/YangYzzzz/image-store@main/fix-dir%2FTypora%2Ftypora-user-images%2F2024%2F10%2F29%2F18-58-47-054e71be37b812bd06a5cb0967386fdf-image-20240117162534495-d115d9.png" alt></p>
<p>在训练神经网络时，在初始化模型参数后， 我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。 注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。 带来的影响之一是我们需要保留中间值，直到反向传播完成。 这也是训练比单纯的预测需要更多的内存（显存）的原因之一。 此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。 因此，使用更大的批量来训练更深层次的网络更容易导致<em>内存不足</em>（out of memory）错误。</p>
<h3 id="6-权重初始化"><a href="#6-权重初始化" class="headerlink" title="6. 权重初始化"></a>6. 权重初始化</h3><ul>
<li>梯度消失和梯度爆炸是深度网络中常见的问题。在参数初始化时需要非常小心，以确保梯度和参数可以得到很好的控制。</li>
<li>ReLU激活函数缓解了梯度消失问题，这样可以加速收敛。</li>
</ul>
<h4 id="6-1-默认初始化-Kaiming-初始化"><a href="#6-1-默认初始化-Kaiming-初始化" class="headerlink" title="6.1 默认初始化 / Kaiming 初始化"></a>6.1 默认初始化 / Kaiming 初始化</h4><p>使用标准正态分布来初始化权重值。</p>
<h4 id="6-2-Xavier初始化"><a href="#6-2-Xavier初始化" class="headerlink" title="6.2 Xavier初始化"></a>6.2 Xavier初始化</h4><p>仍然假设采样的分布应为零均值，方差$\delta^2$，并且假设在一个不存在非线性的全连接层中，输入$x$也具有零均值和方差$\gamma^2$，我们此时期望输出$o$与输入$x$的均值方差保持一致（最重要的<strong>数值稳定性</strong>）即具有零均值与方差$\gamma^2$，经过数学推导可得$n<em>\mathrm{in} \sigma^2 = 1$。在反向传播中运用相同的推论（此部分较复杂），我们希望梯度的方差也保持不变，因此$n</em>\mathrm{out} \sigma^2 = 1$。因此，折中方案即令$\sigma^2 = \frac{2}{n<em>{in}+n</em>{out}}$。</p>
<p>注意这并不代表一定是正态分布采样，实际上主流有两种方式采样，一种是正态分布采样，一种是均匀分布采样，等价于值域$U\left(-\sqrt{\frac{6}{n<em>\mathrm{in} + n</em>\mathrm{out}}}, \sqrt{\frac{6}{n<em>\mathrm{in} + n</em>\mathrm{out}}}\right)$。</p>
<h2 id="五-深度学习计算"><a href="#五-深度学习计算" class="headerlink" title="五. 深度学习计算"></a>五. 深度学习计算</h2><h3 id="1-层和块"><a href="#1-层和块" class="headerlink" title="1. 层和块"></a>1. 层和块</h3><p>块中包含多个层或块，为了复用等操作，均继承自nn.Module。</p>
<p>顺序块是一种特殊的块，它内部的层链条般顺序相连。</p>
<p>我们也可以自定义块，在块内部实现任意的数据流动，我们也可以在其中设置不反向传播的参数等。</p>
<h3 id="2-参数管理"><a href="#2-参数管理" class="headerlink" title="2. 参数管理"></a>2. 参数管理</h3><h4 id="2-1-获取参数"><a href="#2-1-获取参数" class="headerlink" title="2.1 获取参数"></a>2.1 获取参数</h4><p>获取全部参数，无论哪个层次，均可通过下述方法获得当前层次包含的全部参数，因此我们可以粗或细粒度批量操作参数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for name, param in net.named_parameters()  # 带名称</span><br><span class="line">for param in net.parameters()  # 不带名称</span><br></pre></td></tr></table></figure>
<p>网络图如图所示，我们可以通过索引获得相应的参数，如我们访问第一个主要的块中、第二个子块的第一层的偏置项。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rgnet[<span class="number">0</span>][<span class="number">1</span>][<span class="number">0</span>].bias.data</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (0): Sequential(</span><br><span class="line">    (block 0): Sequential(</span><br><span class="line">      (0): Linear(in_features=4, out_features=8, bias=True)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(in_features=8, out_features=4, bias=True)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block 1): Sequential(</span><br><span class="line">      (0): Linear(in_features=4, out_features=8, bias=True)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(in_features=8, out_features=4, bias=True)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block 2): Sequential(</span><br><span class="line">      (0): Linear(in_features=4, out_features=8, bias=True)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(in_features=8, out_features=4, bias=True)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block 3): Sequential(</span><br><span class="line">      (0): Linear(in_features=4, out_features=8, bias=True)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(in_features=8, out_features=4, bias=True)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (1): Linear(in_features=4, out_features=1, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="2-2-初始化"><a href="#2-2-初始化" class="headerlink" title="2.2 初始化"></a>2.2 初始化</h4><p>各种类型初始化，torch也会默认初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nn.init.normal_(m.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>) nn.init.zeros_(m.bias)</span><br><span class="line">nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">nn.init.xavier_uniform_(m.weight)</span><br></pre></td></tr></table></figure>
<h3 id="3-读写文件"><a href="#3-读写文件" class="headerlink" title="3. 读写文件"></a>3. 读写文件</h3><p><code>state_dict()</code>：权重字典，包含名称(<code>xx.xx.weight/bias</code>)和值(<code>[1., 2., ...]</code>)</p>
<p>这要求我们先初始化好框架，在往框架中填参数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.save(net.state_dict(), &#x27;mlp.params&#x27;)  # 保存权重</span><br><span class="line">clone.load_state_dict(torch.load(&#x27;mlp.params&#x27;))  # 加载权重</span><br></pre></td></tr></table></figure>
<p>当然我们同样可以保存模型与参数。</p>
<h3 id="4-GPU"><a href="#4-GPU" class="headerlink" title="4. GPU"></a>4. GPU</h3><p>只要所有的数据和参数都在同一个设备上， 我们就可以有效地学习模型。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.device(&#x27;cpu&#x27;), torch.device(&#x27;cuda:1&#x27;)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/YangYzzzz/image-store@main/fix-dir%2FTypora%2Ftypora-user-images%2F2024%2F10%2F29%2F18-59-30-162961a8dfb413d1d1c69f84a4d03101-image-20240119162731561-4f7c18.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x.cpu() <span class="comment"># gpu -&gt; cpu</span></span><br><span class="line">x.to(device)/x.cuda() <span class="comment"># cpu -&gt; gpu</span></span><br></pre></td></tr></table></figure>
<ul>
<li>我们可以指定用于存储和计算的设备，例如CPU或GPU。默认情况下，数据在主内存中创建，然后使用CPU进行计算。</li>
<li>深度学习框架要求计算的所有输入数据都在同一设备上，无论是CPU还是GPU。</li>
<li>不经意地移动数据可能会显著降低性能。一个典型的错误如下：计算GPU上每个小批量的损失，并在命令行中将其报告给用户（或将其记录在NumPy <code>ndarray</code>中）时，将触发全局解释器锁，从而使所有GPU阻塞。最好是为GPU内部的日志分配内存，并且只移动较大的日志! </li>
</ul>
<h2 id="六-卷积神经网络"><a href="#六-卷积神经网络" class="headerlink" title="六. 卷积神经网络"></a>六. 卷积神经网络</h2><p>我们若对图像也采用同样的全连接层，则每个输出值要收到所有输入像素的信息，这样参数开销是巨大的。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/YangYzzzz/image-store@main/fix-dir%2FTypora%2Ftypora-user-images%2F2024%2F10%2F29%2F18-07-55-ccee1874415d610631937a0e9ffec931-image-20240119170233285-456416.png" alt="image-20240119170233285"></p>
<h3 id="1-计算机视觉的两大特性"><a href="#1-计算机视觉的两大特性" class="headerlink" title="1. 计算机视觉的两大特性"></a>1. 计算机视觉的两大特性</h3><ol>
<li><em>平移不变性</em>（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。</li>
<li><em>局部性</em>（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</li>
</ol>
<h3 id="2-卷积层"><a href="#2-卷积层" class="headerlink" title="2. 卷积层"></a>2. 卷积层</h3><h4 id="2-1-平移不变性"><a href="#2-1-平移不变性" class="headerlink" title="2.1  平移不变性"></a>2.1  平移不变性</h4><p>这意味着检测对象随输入的平移，应该仅导致隐藏表示H的平移，即在<code>(i,j)</code>位置的输出像素和在<code>(a, b)</code>位置的输出像素所使用的参数应当是相同的，即参数不依赖于<code>(i,j)</code>的值。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/YangYzzzz/image-store@main/fix-dir%2FTypora%2Ftypora-user-images%2F2024%2F10%2F29%2F18-59-58-d56fd648409f4fbefc88cb0bdd3b5a79-image-20240119170642333-b19046.png" alt></p>
<h4 id="2-2-局部性"><a href="#2-2-局部性" class="headerlink" title="2.2 局部性"></a>2.2 局部性</h4><p>划定一个范围，卷积核只看范围之内的内容，而忽视之外的内容。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/YangYzzzz/image-store@main/fix-dir%2FTypora%2Ftypora-user-images%2F2024%2F10%2F29%2F19-00-20-ea6b0e4bf441b32718a6357a079e44c9-image-20240119170811852-13a926.png" alt></p>
<h4 id="2-3-通道"><a href="#2-3-通道" class="headerlink" title="2.3 通道"></a>2.3 通道</h4><p>图象是三维的，隐藏表示也最好用三维向量，换句话说，对于每一个空间位置，我们想要采用一组而不是一个隐藏表示。这样一组隐藏表示可以想象成一些互相堆叠的二维网格。 因此，我们可以把隐藏表示想象为一系列具有二维张量的<em>通道</em>（channel）。 这些通道有时也被称为<em>特征映射</em>（feature maps），因为每个通道都向后续层提供一组空间化的学习特征。</p>
<p>考虑到输入的第三维度和输出的第三维度，我们的卷积核(卷积层的参数)拥有四个维度。</p>
<p>其中隐藏表示H中的索引d表示输出通道，而随后的输出将继续以三维张量H作为输入进入下一个卷积层。 所以，该公式可以定义具有多个通道的卷积层，而其中V是该卷积层的权重。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/YangYzzzz/image-store@main/fix-dir%2FTypora%2Ftypora-user-images%2F2024%2F10%2F29%2F18-07-55-5d21528e7551121ddc3fabd6e287f90a-image-20240119171019867-1e13a5.png" alt="image-20240119171019867"></p>
<h4 id="2-4-归纳偏置"><a href="#2-4-归纳偏置" class="headerlink" title="2.4 归纳偏置"></a>2.4 归纳偏置</h4><p>上述的图像假设均为先验知识，若不符合归纳偏置，则效果可能会很差，即用归纳偏置换更少的参数与运行训练效率。</p>
<h4 id="2-5-图像卷积"><a href="#2-5-图像卷积" class="headerlink" title="2.5 图像卷积"></a>2.5 图像卷积</h4><p>在卷积神经网络中，对于某一层的任意元素，其<em>感受野</em>（receptive field）是指在前向传播期间可能影响计算的所有元素（来自所有先前层）。</p>
<p>我们可以从数据中学习卷积核的参数。</p>
<h4 id="2-6-关键参数，填充和步幅，输入输出通道"><a href="#2-6-关键参数，填充和步幅，输入输出通道" class="headerlink" title="2.6 关键参数，填充和步幅，输入输出通道"></a>2.6 关键参数，填充和步幅，输入输出通道</h4><ul>
<li><strong>padding</strong>: 一般为保持输入和输出的长宽相同(<code>stride=1</code>)，因此通常填充方式为。做padding的目的是防止边缘信息丢失。</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/YangYzzzz/image-store@main/fix-dir%2FTypora%2Ftypora-user-images%2F2024%2F10%2F29%2F19-00-51-17e6c09f9283ef74d579b40ffad0ddda-image-20240119174615738-8637ab.png" alt></p>
<ul>
<li><strong>stride</strong>: 用于减小输出的长宽，通常而言，步长为n，我们的输出长宽均除以n。</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/YangYzzzz/image-store@main/fix-dir%2FTypora%2Ftypora-user-images%2F2024%2F10%2F29%2F19-01-03-411f5a3435081d43baf42f5aa168599c-image-20240119174811722-8f72f0.png" alt></p>
<ul>
<li><strong>in, out</strong>: 输入输出通道，卷积核拥有参数 <script type="math/tex">c_in * c_out * k_h * k_w</script> </li>
</ul>
<h4 id="2-7-1x1-卷积核"><a href="#2-7-1x1-卷积核" class="headerlink" title="2.7 1x1 卷积核"></a>2.7 1x1 卷积核</h4><p>作用于同一个位置的不同通道，相当于在通道维度上的全连接层，通常用于调整网络层的通道数量和控制模型复杂性。</p>
<h3 id="3-池化层"><a href="#3-池化层" class="headerlink" title="3. 池化层"></a>3. 池化层</h3><p>通常当我们处理图像时，我们希望逐渐降低隐藏表示的空间分辨率、聚集信息，这样随着我们在神经网络中层叠的上升，每个神经元对其敏感的感受野（输入）就越大。</p>
<p>而我们的机器学习任务通常会跟全局图像的问题有关（例如，“图像是否包含一只猫呢？”），所以我们最后一层的神经元应该对整个输入的全局敏感。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层。</p>
<p>池化层没有可学习参数。</p>
<h4 id="3-1-目的"><a href="#3-1-目的" class="headerlink" title="3.1 目的"></a>3.1 目的</h4><ol>
<li>降低卷积层对位置的敏感性，相邻位置的值被综合为一个值</li>
<li>降低对空间降采样表示的敏感性。</li>
</ol>
<h4 id="3-2-最大池化和平均池化"><a href="#3-2-最大池化和平均池化" class="headerlink" title="3.2 最大池化和平均池化"></a>3.2 最大池化和平均池化</h4><p>torch默认步长等于窗口大小。</p>
<ul>
<li>对于给定输入元素，最大汇聚层会输出该窗口内的最大值，平均汇聚层会输出该窗口内的平均值。</li>
<li>汇聚层的主要优点之一是减轻卷积层对位置的过度敏感。</li>
<li>我们可以指定汇聚层的填充和步幅。</li>
<li>使用最大汇聚层以及大于1的步幅，可减少空间维度（如高度和宽度）。</li>
<li>汇聚层的输出通道数与输入通道数相同。</li>
</ul>
<p><strong>普遍规律</strong>：长宽维度逐渐降低，即全局信息越来越汇总，而通道数逐渐变多，即全局的不同特征被提取的越来越多</p>
<h3 id="4-LeNet"><a href="#4-LeNet" class="headerlink" title="4. LeNet"></a>4. LeNet</h3><p>卷积神经网络的开山之作，作者杨立昆；深度很浅，以全连接层汇聚。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/YangYzzzz/image-store@main/fix-dir%2FTypora%2Ftypora-user-images%2F2024%2F10%2F29%2F19-01-26-0b43f529afaff80c8c77f5b9108c6985-image-20240119180959946-5f442c.png" alt></p>
<h2 id="七-现代神经网络"><a href="#七-现代神经网络" class="headerlink" title="七. 现代神经网络"></a>七. 现代神经网络</h2><h3 id="1-AlexNet"><a href="#1-AlexNet" class="headerlink" title="1. AlexNet"></a>1. AlexNet</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/YangYzzzz/image-store@main/fix-dir%2FTypora%2Ftypora-user-images%2F2024%2F10%2F29%2F19-02-41-f538a0d4f09e5aaa69087c5184227cac-image-20240121184021967-b4b6c9.png" alt></p>
<p>和LeNet比更深了，用的卷积核更大了，且激活函数使用reLU而不是sigmoid。</p>
<p><strong>relu的优点</strong>：简单，不像sigmoid一般有复杂的求幂运算；正向时梯度为1，有效避免了梯度消失的问题，因此目前设计激活函数时均使用relu及其变种，如leakyrelu。</p>
<h3 id="2-VGG"><a href="#2-VGG" class="headerlink" title="2. VGG"></a>2. VGG</h3><p>研究人员开始从单个神经元的角度思考问题，发展到整个层，现在又转向块，重复层的模式。</p>
<p>将多个层堆叠为一个块，一个块通常是可以复用的。</p>
<p>经典卷积神经网络的基本组成部分是下面的这个序列：</p>
<ol>
<li>带填充以保持分辨率的卷积层；</li>
<li>非线性激活函数，如ReLU；</li>
<li>汇聚层，如最大汇聚层。</li>
</ol>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/YangYzzzz/image-store@main/fix-dir%2FTypora%2Ftypora-user-images%2F2024%2F10%2F29%2F18-07-56-6a08c88ee754dc33320c5977fcc63bf2-image-20240121184837606-9f1c66.png" alt="image-20240121184837606"></p>
<p>原始VGG网络有5个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。 第一个模块有64个输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到512。由于该网络使用8个卷积层和3个全连接层，因此它通常被称为VGG-11。</p>
<p>一般激活函数和池化层由于无需参数，不称作一层。</p>
<p>卷积参数少，但是运算次数不会太少。</p>
<h3 id="3-NiN"><a href="#3-NiN" class="headerlink" title="3. NiN"></a>3. NiN</h3><p>目前的设计模式都是通过一系列的卷积层和池化层提取空间结构，然后通过全连接层对特征的表征进行处理。但是在最后使用全连接层会放弃表征的空间结构（即使在深层长宽通常已经很小），因此NiN提出，可以在每个像素的通道上分别使用多层感知机，即<script type="math/tex">1*1</script>的卷积核。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/YangYzzzz/image-store@main/fix-dir%2FTypora%2Ftypora-user-images%2F2024%2F10%2F29%2F19-03-01-a8d922ec58caaeb466a1f9e749d33f0c-image-20240121185710702-cb66cf.png" alt></p>
<p>最后通过一个全局平均池化层，将每个通道的长和宽下采样到<script type="math/tex">1*1</script>。</p>
<h3 id="4-含并行连结的网络（GoogLeNet）"><a href="#4-含并行连结的网络（GoogLeNet）" class="headerlink" title="4. 含并行连结的网络（GoogLeNet）"></a>4. 含并行连结的网络（GoogLeNet）</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/YangYzzzz/image-store@main/fix-dir%2FTypora%2Ftypora-user-images%2F2024%2F10%2F29%2F18-07-58-5abfe391da4a4fead91860879b3274e1-image-20240121212632646-8afc24.png" alt="image-20240121212632646"></p>
<p>它们可以用各种滤波器尺寸探索图像，这意味着不同大小的滤波器可以有效地识别不同范围的图像细节。 同时，我们可以为不同的滤波器分配不同数量的参数。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/YangYzzzz/image-store@main/fix-dir%2FTypora%2Ftypora-user-images%2F2024%2F10%2F29%2F19-03-44-5265ca0098df7c1afaa3b1495fe7db5f-image-20240121213106433-ace719.png" alt></p>
<p>GoogLeNet一共使用9个Inception块和全局平均汇聚层的堆叠来生成其估计值。Inception块之间的最大汇聚层可降低维度。 第一个模块类似于AlexNet和LeNet，Inception块的组合从VGG继承，全局平均汇聚层避免了在最后使用全连接层。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://YangYzzzz.github.io">Yang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://yangyzzzz.github.io/post/866b08a7.html">https://yangyzzzz.github.io/post/866b08a7.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://YangYzzzz.github.io" target="_blank">Yang's CS World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E9%98%85%E8%AF%BB/">阅读</a></div><div class="post_share"><div class="social-share" data-image="/img/123230344_p0_master1200.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/d00a62b9.html" title="『机器学习系统』OpenMLSYS——阅读笔记1"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/fufu4.jpeg" onerror="onerror=null;src='/img/404.jpeg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">『机器学习系统』OpenMLSYS——阅读笔记1</div></div></a></div><div class="next-post pull-right"><a href="/post/32f03788.html" title="『智谱清言』深度学习环境常见问题"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/123230344_p0_master1200.jpeg" onerror="onerror=null;src='/img/404.jpeg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">『智谱清言』深度学习环境常见问题</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/post/52e1e34e.html" title="『深度学习』动手学深度学习——阅读笔记2"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/123182059_p0_master1200.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-19</div><div class="title">『深度学习』动手学深度学习——阅读笔记2</div></div></a></div><div><a href="/post/32f03788.html" title="『智谱清言』深度学习环境常见问题"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/123230344_p0_master1200.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-14</div><div class="title">『智谱清言』深度学习环境常见问题</div></div></a></div><div><a href="/post/510af8c4.html" title="『智谱清言』Linux裸机配置"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/niyan1.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-03</div><div class="title">『智谱清言』Linux裸机配置</div></div></a></div><div><a href="/post/b5b4da6e.html" title="『智谱清言』CogVLM2部署实践"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/118894612_p0_master1700.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-30</div><div class="title">『智谱清言』CogVLM2部署实践</div></div></a></div><div><a href="/post/d5823525.html" title="『机器学习系统』Deepseekv2"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/119780723_p0_master1800.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-07</div><div class="title">『机器学习系统』Deepseekv2</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Yang</div><div class="author-info__description">友链请私戳我</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">51</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">20</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/YangYzzzz"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/YangYzzzz" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:21373037@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://blog.csdn.net/weixin_71968907" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">青山依旧在，几度夕阳红</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E3%80%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8F%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B01"><span class="toc-number">1.</span> <span class="toc-text">『深度学习』动手学深度学习——阅读笔记1</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%B6-%E5%89%8D%E8%A8%80"><span class="toc-number">1.1.</span> <span class="toc-text">零. 前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80-%E5%BC%95%E8%A8%80"><span class="toc-number">1.2.</span> <span class="toc-text">一.引言</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%9B%9B%E5%A4%A7%E5%9F%BA%E7%9F%B3"><span class="toc-number">1.2.1.</span> <span class="toc-text">1. 四大基石</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB"><span class="toc-number">1.2.2.</span> <span class="toc-text">2. 机器学习分类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80"><span class="toc-number">1.3.</span> <span class="toc-text">二. 数学基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="toc-number">1.3.1.</span> <span class="toc-text">1. 线性代数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%BE%AE%E5%88%86"><span class="toc-number">1.3.2.</span> <span class="toc-text">2. 微分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%A6%82%E7%8E%87"><span class="toc-number">1.3.3.</span> <span class="toc-text">3. 概率</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89-%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.4.</span> <span class="toc-text">三. 线性神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.4.1.</span> <span class="toc-text">1. 线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82"><span class="toc-number">1.4.2.</span> <span class="toc-text">2. 全连接层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-softmax"><span class="toc-number">1.4.3.</span> <span class="toc-text">3.  softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">1.4.4.</span> <span class="toc-text">4. 仿射变换的概念</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">1.5.</span> <span class="toc-text">四. 多层感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE"><span class="toc-number">1.5.1.</span> <span class="toc-text">1. 训练误差与泛化误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%AC%A0%E6%8B%9F%E5%90%88%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">1.5.2.</span> <span class="toc-text">2. 欠拟合与过拟合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%BD%B1%E5%93%8D%E6%A8%A1%E5%9E%8B%E6%B3%9B%E5%8C%96%E7%9A%84%E4%B8%89%E4%B8%AA%E5%9B%A0%E7%B4%A0"><span class="toc-number">1.5.3.</span> <span class="toc-text">3. 影响模型泛化的三个因素</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="toc-number">1.5.4.</span> <span class="toc-text">4. 正则化技术</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-number">1.5.5.</span> <span class="toc-text">5. 训练过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.5.6.</span> <span class="toc-text">6. 权重初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-1-%E9%BB%98%E8%AE%A4%E5%88%9D%E5%A7%8B%E5%8C%96-Kaiming-%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.5.6.1.</span> <span class="toc-text">6.1 默认初始化 &#x2F; Kaiming 初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.5.6.2.</span> <span class="toc-text">6.2 Xavier初始化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97"><span class="toc-number">1.6.</span> <span class="toc-text">五. 深度学习计算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%B1%82%E5%92%8C%E5%9D%97"><span class="toc-number">1.6.1.</span> <span class="toc-text">1. 层和块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%8F%82%E6%95%B0%E7%AE%A1%E7%90%86"><span class="toc-number">1.6.2.</span> <span class="toc-text">2. 参数管理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E8%8E%B7%E5%8F%96%E5%8F%82%E6%95%B0"><span class="toc-number">1.6.2.1.</span> <span class="toc-text">2.1 获取参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.6.2.2.</span> <span class="toc-text">2.2 初始化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6"><span class="toc-number">1.6.3.</span> <span class="toc-text">3. 读写文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-GPU"><span class="toc-number">1.6.4.</span> <span class="toc-text">4. GPU</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.7.</span> <span class="toc-text">六. 卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%9A%84%E4%B8%A4%E5%A4%A7%E7%89%B9%E6%80%A7"><span class="toc-number">1.7.1.</span> <span class="toc-text">1. 计算机视觉的两大特性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">1.7.2.</span> <span class="toc-text">2. 卷积层</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E5%B9%B3%E7%A7%BB%E4%B8%8D%E5%8F%98%E6%80%A7"><span class="toc-number">1.7.2.1.</span> <span class="toc-text">2.1  平移不变性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E5%B1%80%E9%83%A8%E6%80%A7"><span class="toc-number">1.7.2.2.</span> <span class="toc-text">2.2 局部性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-%E9%80%9A%E9%81%93"><span class="toc-number">1.7.2.3.</span> <span class="toc-text">2.3 通道</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE"><span class="toc-number">1.7.2.4.</span> <span class="toc-text">2.4 归纳偏置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5-%E5%9B%BE%E5%83%8F%E5%8D%B7%E7%A7%AF"><span class="toc-number">1.7.2.5.</span> <span class="toc-text">2.5 图像卷积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-6-%E5%85%B3%E9%94%AE%E5%8F%82%E6%95%B0%EF%BC%8C%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85%EF%BC%8C%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="toc-number">1.7.2.6.</span> <span class="toc-text">2.6 关键参数，填充和步幅，输入输出通道</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-7-1x1-%E5%8D%B7%E7%A7%AF%E6%A0%B8"><span class="toc-number">1.7.2.7.</span> <span class="toc-text">2.7 1x1 卷积核</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-number">1.7.3.</span> <span class="toc-text">3. 池化层</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-%E7%9B%AE%E7%9A%84"><span class="toc-number">1.7.3.1.</span> <span class="toc-text">3.1 目的</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-%E6%9C%80%E5%A4%A7%E6%B1%A0%E5%8C%96%E5%92%8C%E5%B9%B3%E5%9D%87%E6%B1%A0%E5%8C%96"><span class="toc-number">1.7.3.2.</span> <span class="toc-text">3.2 最大池化和平均池化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-LeNet"><span class="toc-number">1.7.4.</span> <span class="toc-text">4. LeNet</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83-%E7%8E%B0%E4%BB%A3%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.8.</span> <span class="toc-text">七. 现代神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-AlexNet"><span class="toc-number">1.8.1.</span> <span class="toc-text">1. AlexNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-VGG"><span class="toc-number">1.8.2.</span> <span class="toc-text">2. VGG</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-NiN"><span class="toc-number">1.8.3.</span> <span class="toc-text">3. NiN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E7%BB%93%E7%9A%84%E7%BD%91%E7%BB%9C%EF%BC%88GoogLeNet%EF%BC%89"><span class="toc-number">1.8.4.</span> <span class="toc-text">4. 含并行连结的网络（GoogLeNet）</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/52e1e34e.html" title="『深度学习』动手学深度学习——阅读笔记2"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/123182059_p0_master1200.jpeg" onerror="this.onerror=null;this.src='/img/404.jpeg'" alt="『深度学习』动手学深度学习——阅读笔记2"/></a><div class="content"><a class="title" href="/post/52e1e34e.html" title="『深度学习』动手学深度学习——阅读笔记2">『深度学习』动手学深度学习——阅读笔记2</a><time datetime="2025-01-19T13:44:19.349Z" title="发表于 2025-01-19 21:44:19">2025-01-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/a83ff101.html" title="『信息论』信息论——学习笔记1"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/119780754_p0_master1700.jpeg" onerror="this.onerror=null;this.src='/img/404.jpeg'" alt="『信息论』信息论——学习笔记1"/></a><div class="content"><a class="title" href="/post/a83ff101.html" title="『信息论』信息论——学习笔记1">『信息论』信息论——学习笔记1</a><time datetime="2024-10-30T09:46:33.000Z" title="发表于 2024-10-30 17:46:33">2024-10-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/b2db68a7.html" title="『机器学习系统』OpenMLSYS——阅读笔记2"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/119312426_p0_master1700.jpeg" onerror="this.onerror=null;this.src='/img/404.jpeg'" alt="『机器学习系统』OpenMLSYS——阅读笔记2"/></a><div class="content"><a class="title" href="/post/b2db68a7.html" title="『机器学习系统』OpenMLSYS——阅读笔记2">『机器学习系统』OpenMLSYS——阅读笔记2</a><time datetime="2024-10-29T12:14:02.000Z" title="发表于 2024-10-29 20:14:02">2024-10-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/d00a62b9.html" title="『机器学习系统』OpenMLSYS——阅读笔记1"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/fufu4.jpeg" onerror="this.onerror=null;this.src='/img/404.jpeg'" alt="『机器学习系统』OpenMLSYS——阅读笔记1"/></a><div class="content"><a class="title" href="/post/d00a62b9.html" title="『机器学习系统』OpenMLSYS——阅读笔记1">『机器学习系统』OpenMLSYS——阅读笔记1</a><time datetime="2024-10-29T12:14:00.000Z" title="发表于 2024-10-29 20:14:00">2024-10-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/866b08a7.html" title="『深度学习』动手学深度学习——阅读笔记1"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/123230344_p0_master1200.jpeg" onerror="this.onerror=null;this.src='/img/404.jpeg'" alt="『深度学习』动手学深度学习——阅读笔记1"/></a><div class="content"><a class="title" href="/post/866b08a7.html" title="『深度学习』动手学深度学习——阅读笔记1">『深度学习』动手学深度学习——阅读笔记1</a><time datetime="2024-10-29T10:04:51.000Z" title="发表于 2024-10-29 18:04:51">2024-10-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By Yang</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">You are not too early，you are not too late，you are very much on time in your time zone</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat-btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.8/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const initGitalk = () => {
    const gitalk = new Gitalk(Object.assign({
      clientID: 'Ov23liK7Xv2gk4Mj8hqI',
      clientSecret: 'a380272ec98afefe58e4fdfacaa66c9eb159e415',
      repo: 'BlogComment',
      owner: 'YangYzzzz',
      admin: ['YangYzzzz'],
      id: '4af40a63bdf0fe840402987845d3e2b4',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async() => {
    if (typeof Gitalk === 'function') initGitalk()
    else {
      await getCSS('https://cdn.jsdelivr.net/npm/gitalk@1.8.0/dist/gitalk.min.css')
      await getScript('https://cdn.jsdelivr.net/npm/gitalk@1.8.0/dist/gitalk.min.js')
      initGitalk()
    }
  }
  
  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  if ('Gitalk' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><div class="aplayer no-destroy" data-id="2108205877" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="true"> </div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-heart.min.js" async="async" mobile="true"></script><script>(() => {
  const isChatBtn = true
  const isChatHideShow = false

  if (isChatBtn) {
    const close = () => {
      Chatra('minimizeWidget')
      Chatra('hide')
    }

    const open = () => {
      Chatra('openChat', true)
      Chatra('show')
    }

    window.ChatraSetup = {
      startHidden: true
    }
  
    window.chatBtnFn = () => {
      const isShow = document.getElementById('chatra').classList.contains('chatra--expanded')
      isShow ? close() : open()
    }
  } else if (isChatHideShow) {
    window.chatBtn = {
      hide: () => {
        Chatra('hide')
      },
      show: () => {
        Chatra('show')
      }
    }
  }

  (function(d, w, c) {
    w.ChatraID = 'Xjgb72cDfiFJXzFty'
    var s = d.createElement('script')
    w[c] = w[c] || function() {
        (w[c].q = w[c].q || []).push(arguments)
    }
    s.async = true
    s.src = 'https://call.chatra.io/chatra.js'
    if (d.head) d.head.appendChild(s)
  })(document, window, 'Chatra')

})()</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>